from transformers import MllamaForConditionalGeneration, AutoProcessor
from PIL import Image
import torch
from huggingface_hub import login
from configs.config_reader import *

def predict(image: Image.Image) -> str:
    """
    Processes an image and generates a detailed description using the Llama Visual Instruct model.
    
    This function takes in the path of an image, loads the pre-trained Llama model and processor
    from Hugging Face, and generates a descriptive output. The model is instructed to provide a
    response with approximately 300 words, offering an in-depth description of the content in the image.
    
    Parameters:
        image_path (str): The path to the image file to be processed.
        
    Returns:
        str: A detailed description of the image content generated by the model.
    
    Raises:
        Exception: If there's an error in loading the model, processor, or processing the image.
    """
    # Read configuration values from the .yml file
    configs = read_config()
    token = configs['api']['huggingface_token']
    model_id = configs['model_id']

    # Log in to Hugging Face API for model access
    login(token)

    # Initialize the processor and model for generating image descriptions
    processor = AutoProcessor.from_pretrained(model_id)
    model = MllamaForConditionalGeneration.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )

    # Check for GPU availability and assign the device accordingly
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # Process image for model input
    inputs = processor(images=image, return_tensors="pt").to(device)  # Process image for model input

    # Define a prompt for the model to generate a detailed description
    prompt = "Provide a detailed description of the following image in approximately 300 words."
    # Convert prompt text to input tensor for the model
    inputs['input_ids'] = processor(text=prompt, return_tensors="pt").input_ids.to(inputs["pixel_values"].device)

    # Generate a detailed description from the model
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=350,            # Limit to allow for approximately 300 words in the output
        no_repeat_ngram_size=3,        # Prevent repetitive phrases in the generated output
        early_stopping=True,           # Stop generation once the model reaches a coherent conclusion
    )

    # Decode the generated token IDs to readable text
    description = processor.decode(generated_ids[0], skip_special_tokens=True)

    return description

